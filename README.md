# ğŸš€ Real-Time Crypto Pipeline â€” API > Kafka > Spark > Dashboard

**Author:** [Your Name]  
**Course:** Real-Time Data Processing  
**Stack:** Python Â· Apache Kafka Â· Apache Spark Structured Streaming Â· Streamlit

---

## ğŸ“Œ Business Scenario

This project implements a **real-time cryptocurrency price monitoring platform** for the e-commerce / financial markets scenario.

The platform continuously tracks **BTC and ETH prices** from the Binance public API, processes them through a streaming pipeline, and exposes live KPIs on an interactive dashboard.

**Business objectives:**
- Monitor price evolution in real time (every 5 seconds)
- Compute windowed statistics: average, min, max price per symbol
- Detect price spreads and volatility across sliding time windows
- Provide a live dashboard usable by trading or data teams

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP GET      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     JSON events    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Binance REST   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚   api_producer   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Kafka              â”‚
â”‚  API (public)   â”‚   every 5 sec    â”‚   (Python)        â”‚                    â”‚  Topic: crypto-topicâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                                         â”‚ readStream
                                                                                         â–¼
                                                                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                             â”‚  Spark Structured       â”‚
                                                                             â”‚  Streaming              â”‚
                                                                             â”‚  - from_json            â”‚
                                                                             â”‚  - withWatermark 2min   â”‚
                                                                             â”‚  - window 5min / 1min   â”‚
                                                                             â”‚  - avg / min / max      â”‚
                                                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                                         â”‚ writeStream
                                                                                         â–¼
                                                                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                             â”‚  Kafka                  â”‚
                                                                             â”‚  Topic: crypto-aggregatesâ”‚
                                                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                                         â”‚ KafkaConsumer
                                                                                         â–¼
                                                                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                             â”‚  Streamlit Dashboard    â”‚
                                                                             â”‚  - KPI metrics          â”‚
                                                                             â”‚  - Time series chart    â”‚
                                                                             â”‚  - Bar chart            â”‚
                                                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”Œ API Reference

| Field | Value |
|---|---|
| Provider | [Binance](https://binance.com) |
| Endpoint | `https://api.binance.com/api/v3/ticker/price?symbol={SYMBOL}` |
| Auth | None (public) |
| Rate limit | 1,200 requests/min |
| Format | JSON |
| Symbols used | `BTCUSDT`, `ETHUSDT` |

**Example response:**
```json
{"symbol": "BTCUSDT", "price": "97432.15"}
```

**Normalized event sent to Kafka:**
```json
{
  "symbol": "BTCUSDT",
  "price": 97432.15,
  "event_time": "2025-01-15T14:32:01.123456"
}
```

---

## ğŸ“Š KPIs Displayed

| KPI | Description | Window |
|---|---|---|
| `avg_price` | Average price per symbol | 5 min sliding / 1 min step |
| `min_price` | Minimum price per symbol | 5 min sliding / 1 min step |
| `max_price` | Maximum price per symbol | 5 min sliding / 1 min step |
| `count` | Number of events received | 5 min sliding / 1 min step |
| `spread` | max_price âˆ’ min_price (volatility proxy) | Latest window |

---

## ğŸ“ Project Structure

```
realtime-crypto-pipeline/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ crypto-aggregates/     # Spark fault-tolerance checkpoints
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml     # Kafka + Zookeeper setup
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api_producer.py        # Binance API â†’ Kafka producer
â”‚   â”œâ”€â”€ streaming_app.py       # Spark Structured Streaming job
â”‚   â””â”€â”€ dashboard.py           # Streamlit real-time dashboard
â”œâ”€â”€ logs/                      # Runtime logs (auto-created)
â”œâ”€â”€ start.sh                   # One-command pipeline startup
â”œâ”€â”€ stop.sh                    # One-command pipeline shutdown
â”œâ”€â”€ .env.example               # Environment variable template
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Prerequisites

- Docker & Docker Compose
- Python 3.9+
- Apache Spark 3.5+ with `spark-submit` in PATH
- Java 11 or 17 (required by Spark)

---

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/<your-username>/realtime-crypto-pipeline.git
cd realtime-crypto-pipeline
```

### 2. Install Python dependencies
```bash
python3 -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Configure environment
```bash
cp .env.example .env
# No API key needed â€” Binance endpoint is public
```

### 4. Launch everything
```bash
chmod +x start.sh stop.sh
bash start.sh
```

This will automatically:
1. Start Kafka & Zookeeper via Docker
2. Create Kafka topics (`crypto-topic`, `crypto-aggregates`)
3. Start the API producer
4. Start the Spark streaming job
5. Start the Streamlit dashboard

### 5. Open the dashboard
```
http://localhost:8501
```

### 6. Monitor Spark
```
http://localhost:4040
```

### 7. Stop everything
```bash
bash stop.sh
```

---

## ğŸ”§ Manual Startup (step by step)

If you prefer to run each component separately:

```bash
# Terminal 1 â€” Kafka
docker compose -f docker/docker-compose.yml up

# Terminal 2 â€” API Producer
python3 src/api_producer.py

# Terminal 3 â€” Spark job
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  src/streaming_app.py

# Terminal 4 â€” Dashboard
streamlit run src/dashboard.py
```

---

## ğŸ›¡ï¸ Technical Decisions

### Watermark â€” 2 minutes
The API is polled every 5 seconds with very low network latency. A 2-minute watermark is sufficient to handle any delayed events while keeping state memory low.

### Sliding window â€” 5 min / 1 min
A 5-minute window provides enough data points for meaningful statistics. The 1-minute slide gives frequent updates without overloading Spark.

### `outputMode("update")`
Used instead of `complete` to avoid rewriting all historical windows on every micro-batch â€” more efficient for long-running jobs.

### Checkpoint
Located at `./checkpoints/crypto-aggregates`. Allows the Spark job to resume exactly where it left off after a restart, without reprocessing old data.

### Session state in Streamlit
The Kafka consumer is recreated on every Streamlit rerun (every 5s). Session state accumulates records across reruns so the charts grow over time rather than resetting.

---

## ğŸ“¸ Spark UI â€” Monitoring

Access the Spark UI at `http://localhost:4040` while the streaming job is running.

### Dashboard â€” Live KPIs (BTC & ETH)
![Dashboard KPIs](docs/dashboard_kpi.png)

> Real-time KPI cards showing avg price, spread, min, max and event count for the latest window.

### Dashboard â€” Average Price Time Series
![Dashboard Time Series](docs/dashboard_timeseries.png)

> Sliding window (5 min / 1 min) average price evolution for BTC and ETH.

### Dashboard â€” Price Range Bar Chart
![Dashboard Bar Chart](docs/dashboard_barchart.png)

> Min / Avg / Max price comparison per symbol for the latest window.

### Spark UI â€” Jobs Tab
![Spark Jobs](docs/spark_jobs.png)

> 33 completed jobs, each taking ~0.8s. All stages succeeded (2/2), confirming stable pipeline execution.

### Spark UI â€” Structured Streaming Tab
![Spark Streaming](docs/spark_streaming.png)

> Micro-batches trigger every 10 seconds. Processing time stays well under the trigger interval.

### Spark UI â€” Stages Tab
![Spark Stages](docs/screenshots/spark_stages.png)

> Each stage shows the Kafka read â†’ JSON parse â†’ watermark â†’ window aggregation â†’ Kafka write pipeline.

---

## âš ï¸ Limitations & Possible Improvements

| Limitation | Improvement |
|---|---|
| Single Kafka broker (no replication) | Add multiple brokers for production fault tolerance |
| Binance free tier â€” no historical data | Add batch backfill from a historical data source |
| Dashboard reloads full Kafka topic on restart | Persist aggregates to Parquet or a database |
| No authentication on dashboard | Add Streamlit authentication |
| No alerting | Add price spike detection and email/Slack alerts |
| Manual Spark submit | Containerize Spark job in Docker Compose |

---

## ğŸ“¦ Dependencies

Key packages (see `requirements.txt` for full list):

| Package | Version | Purpose |
|---|---|---|
| `kafka-python` | 2.3.0 | Kafka producer & consumer |
| `pyspark` | 3.5.x | Spark Structured Streaming |
| `streamlit` | latest | Real-time dashboard |
| `plotly` | latest | Interactive charts |
| `streamlit-autorefresh` | latest | Auto-refresh widget |
| `requests` | 2.25.1 | Binance API calls |